model_name: "openai/whisper-small"
metadata_path: "/home/u917/PROJECT/aphasia/data_concat/concat_dataset.csv"
enable_wandb: False
enable_optuna: False
wandb:
  key: "50c2664133a05f17cba29eec0576ab3905ed2ec9"
  project: "whisper_finetuned_aphasia_cat"
  entity: "monirome_t"
#  mode: "online"
#  save_code: False
#  log_model: False
training_args:
  num_train_epochs: 5
  output_dir: "./whisper_training_output"
  gradient_checkpointing: True
  per_device_train_batch_size: 8
  gradient_accumulation_steps: 2
  per_device_eval_batch_size: 8
  learning_rate: 3e-5
  warmup_steps: 500
  fp16: True
  max_steps: 3000  
  evaluation_strategy: "steps"
  save_total_limit: 5
  predict_with_generate: True
  generation_max_length: 50
  logging_steps: 200
  save_steps: 100
  eval_steps: 100
  report_to: "wandb"
  load_best_model_at_end: True
  metric_for_best_model: "wer"
  greater_is_better: False
  optim: "adamw_hf"
  lr_scheduler_type: "linear"
  adam_beta1: 0.9
  adam_beta2: 0.999
#  adam_epsilon: 1e-8
  weight_decay: 0.01
  max_grad_norm: 1.0
evaluation_args:
  evaluation_strategy: "steps"
  eval_steps: 1000  # Aumentado de 500 a 1000
  logging_dir: "./logs"
  save_total_limit: 3
  load_best_model_at_end: True
  metric_for_best_model: "wer"
  greater_is_better: False
  predict_with_generate: True
  generation_max_length: 50